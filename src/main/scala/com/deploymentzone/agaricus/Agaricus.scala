package com.deploymentzone.agaricus

import com.deploymentzone.agaricus.io.{Attributes, Data}
import com.deploymentzone.agaricus.ml.SVM
import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.sql.SparkSession

/**
  * Agaricus Lepiota driver.
  */
object Agaricus {
  private val attributes = Attributes()
  private val svm = SVM.initialize(attributes)
  private val spark = SparkSession.builder.appName("Agaricus Lepiota").getOrCreate()

  def model(): LogisticRegressionModel = {


    import spark.implicits._
    val data = spark.sparkContext.parallelize(Data()).map(svm.toLabeledPoint)

    val Array(training, test) = data.randomSplit(Array(8.0, 2.0))

    val lr = new LogisticRegression()
        .setTol(0.1)

    val model: LogisticRegressionModel = lr.fit(training.toDF)

    val predictions = model.evaluate(test.toDF).predictions
    val metrics = new MulticlassMetrics(predictions.select($"prediction", $"label").map {
      row => (row.getDouble(0), row.getDouble(1))
    }.rdd)

    println(f"Model Accuracy: ${metrics.accuracy * 100.0}%2.2f%%")

    model
  }

  /**
    * Take any example from the agaricus-lepiota.data and check it against the model generated by Agaricus#model.
    *
    * @param str a line from agaricus-lepiota.data including starting classification token e/p
    */
  def paste(model: LogisticRegressionModel, str: String): Unit = {
    import spark.implicits._

    val incoming = spark.sparkContext.parallelize(Array(svm.toLabeledPoint(str))).toDF

    val row = model.evaluate(incoming).predictions.select($"label", $"prediction")
    val (prediction, label) = row.map { r => (r.getDouble(0), r.getDouble(1)) }.first()

    println(s"Result: ${ediblePoisonous(prediction)} (label was: ${ediblePoisonous(label)})")
  }

  private def ediblePoisonous(value: Double): String = value match {
    case 1.0 => "Edible"
    case _ => "Poisonous"
  }
}
